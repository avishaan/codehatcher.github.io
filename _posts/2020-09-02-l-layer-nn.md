---
title: "L-layer Neural Network" 
excerpt: "Quick reference on a generalized network setup based on number of layers"
categories:
  - machine learning
tags:
  - ml
  - reference
  - math
  - neural networks
toc: true
toc_sticky: true
date: 2020-8-29
---
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [ ['$$', '$$'], ['\\[', '\\]'] ],
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

## Intro
This quick reference shows the layout of a neural network to help keep the math consistent.

## Initialization Parameters
Assume \\( n^{[l]} \\) is the number of units in layer \\( l \\) and \\( L \\) is total number of layers. For a given input \\( \mathbf{X} \in \mathbb{R}^{12288 \times 209} \\) and \\( m = 209 \\) training examples then the initialization parameters:

### Math
<div markdown='1'>
<table style="width:100%">
<tbody>
  <tr>
    <td> </td>
    <td><b>Shape of W</b> </td>
    <td><b>Shape of b</b> </td>
    <td><b>Activation</b> </td>
    <td><b>Shape of Activation</b> </td>
  </tr>
  <tr>
    <td><b>Layer 1</b> </td>
    <td> $(n^{[1]},12288)$ </td>
    <td> $(n^{[1]},1)$ </td>
    <td> $Z^{[1]} = W^{[1]} X + b^{[1]} $ </td>
    <td> $(n^{[1]},209)$ </td>
  </tr>
  <tr>
    <td><b>Layer 2</b> </td>
    <td> $(n^{[2]}, n^{[1]})$ </td>
    <td> $(n^{[2]},1)$ </td>
    <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td>
    <td> $(n^{[2]}, 209)$ </td>
  </tr>
  <tr>
    <td> $\vdots$ </td>
    <td> $\vdots$ </td>
    <td> $\vdots$ </td>
    <td> $\vdots$</td>
    <td> $\vdots$ </td>
  </tr>
  <tr>
    <td><b>Layer L-1</b> </td>
    <td> $(n^{[L-1]}, n^{[L-2]})$ </td>
    <td> $(n^{[L-1]}, 1)$ </td>
    <td>$Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td>
    <td> $(n^{[L-1]}, 209)$ </td>
  </tr>
  <tr>
    <td><b>Layer L</b> </td>
    <td> $(n^{[L]}, n^{[L-1]})$ </td>
    <td> $(n^{[L]}, 1)$ </td>
    <td> $Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]}$</td>
    <td> $(n^{[L]}, 209)$ </td>
  </tr>
  </tbody>
</table>
</div>

### Pseudocode
```python
L = len(layer_dims)            # number of layers in the network
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
```

## Activation
We assume $\sigma$ for this example but replace with any activation function. Some ideas for activation functions are listed  
[in this post.]({% post_url 2020-08-31-activation-functions %})

\\[  A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})\\]

keeping in mind for the last layer often the notional is

\\[ A^{[L]} = \hat{Y} \\]


## Cost
This is cross-entropy cost $J$ but others available
\\[ \frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)}) \log\left(1- a^{[L] (i)}\right) \\]

## Backprop
Backprop can be error-prone to implement at times. Use a numeric gradient checking method as a check but make sure to only use the analytic method for performance.

\\[ dW^{[l]} = \frac{\partial \mathcal{J} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}  \\]
\\[  db^{[l]} = \frac{\partial \mathcal{J} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l] (i)}\\]
\\[  A^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\]

## Outro
Once you pass backprop, it's relatively smooth sailing through there through the updates so the rest is omitted for brevity.